{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051df30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef4c26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from config.config import config\n",
    "from models.swin_transformer import SwinTransformerSegmentation\n",
    "from models.cyclegan import Generator\n",
    "from utils.visualization import (\n",
    "    visualize_segmentation,\n",
    "    visualize_comparison,\n",
    "    visualize_attention_maps,\n",
    "    plot_training_curves\n",
    ")\n",
    "from utils.metrics import evaluate_segmentation\n",
    "from data.dataset import get_transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9dba0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load segmentation model\n",
    "seg_model = SwinTransformerSegmentation(\n",
    "    img_size=256,\n",
    "    num_classes=4,\n",
    "    embed_dim=96,\n",
    "    depths=[2, 2, 6, 2],\n",
    "    num_heads=[3, 6, 12, 24]\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = '../checkpoints/best_segmentation.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    seg_model.load_state_dict(checkpoint['seg_model'])\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "    print(f\"Best Dice Score: {checkpoint.get('dice_score', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "seg_model.eval()\n",
    "\n",
    "# Load CycleGAN generators\n",
    "G_s2t = Generator().to(device)\n",
    "G_t2s = Generator().to(device)\n",
    "\n",
    "cyclegan_checkpoint = '../checkpoints/cyclegan_epoch_100.pth'\n",
    "if os.path.exists(cyclegan_checkpoint):\n",
    "    cyclegan_ckpt = torch.load(cyclegan_checkpoint, map_location=device)\n",
    "    G_s2t.load_state_dict(cyclegan_ckpt['G_s2t'])\n",
    "    G_t2s.load_state_dict(cyclegan_ckpt['G_t2s'])\n",
    "    print(f\"Loaded CycleGAN from {cyclegan_checkpoint}\")\n",
    "\n",
    "G_s2t.eval()\n",
    "G_t2s.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15831c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load test images\n",
    "source_img_path = '../data/t1/images/sample_001.png'\n",
    "source_mask_path = '../data/t1/masks/sample_001.png'\n",
    "target_img_path = '../data/t2/images/sample_001.png'\n",
    "\n",
    "# Transforms\n",
    "transform = get_transforms(is_train=False)\n",
    "\n",
    "# Load and preprocess\n",
    "source_img = Image.open(source_img_path).convert('RGB')\n",
    "source_mask = Image.open(source_mask_path).convert('L')\n",
    "target_img = Image.open(target_img_path).convert('RGB')\n",
    "\n",
    "source_tensor = transform(source_img).unsqueeze(0).to(device)\n",
    "target_tensor = transform(target_img).unsqueeze(0).to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    pred_source = seg_model(source_tensor)\n",
    "    pred_target = seg_model(target_tensor)\n",
    "    \n",
    "    # Generate translated images\n",
    "    source_to_target = G_s2t(source_tensor)\n",
    "    target_to_source = G_t2s(target_tensor)\n",
    "\n",
    "# Visualize\n",
    "class_names = ['Background', 'WT', 'TC', 'ET']\n",
    "\n",
    "visualize_segmentation(\n",
    "    source_tensor[0],\n",
    "    torch.from_numpy(np.array(source_mask)),\n",
    "    pred_source[0],\n",
    "    class_names=class_names,\n",
    "    save_path='../outputs/source_prediction.png'\n",
    ")\n",
    "\n",
    "print(\"Source domain prediction saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194b1b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "visualize_comparison(\n",
    "    source_tensor[0],\n",
    "    target_tensor[0],\n",
    "    source_to_target[0],\n",
    "    target_to_source[0],\n",
    "    pred_source[0],\n",
    "    pred_target[0],\n",
    "    torch.from_numpy(np.array(source_mask)),\n",
    "    class_names=class_names,\n",
    "    save_path='../outputs/domain_comparison.png'\n",
    ")\n",
    "\n",
    "print(\"Domain comparison saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a9005",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get attention maps from the model\n",
    "with torch.no_grad():\n",
    "    _ = seg_model(source_tensor)\n",
    "    attention_masks = seg_model.get_attention_masks()\n",
    "\n",
    "if len(attention_masks) > 0:\n",
    "    # Visualize first layer attention\n",
    "    visualize_attention_maps(\n",
    "        attention_masks[0][0],  # First sample, first attention layer\n",
    "        save_path='../outputs/attention_maps.png'\n",
    "    )\n",
    "    print(\"Attention maps saved!\")\n",
    "else:\n",
    "    print(\"No attention masks available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea86bb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_dir = Path('../data/t2/images')\n",
    "test_images = list(test_dir.glob('*.png'))[:10]  # Evaluate on 10 images\n",
    "\n",
    "all_dice_scores = []\n",
    "\n",
    "for img_path in test_images:\n",
    "    # Load image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        pred = seg_model(img_tensor)\n",
    "    \n",
    "    # Note: In real evaluation, you need ground truth masks\n",
    "    # For demonstration, we'll skip actual metrics computation\n",
    "    print(f\"Processed: {img_path.name}\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb546a59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize multiple predictions in a grid\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "test_images = list(Path('../data/t2/images').glob('*.png'))[:4]\n",
    "\n",
    "for idx, img_path in enumerate(test_images):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = seg_model(img_tensor)\n",
    "        pred_class = torch.argmax(pred[0], dim=0).cpu().numpy()\n",
    "    \n",
    "    # Denormalize image\n",
    "    img_display = img_tensor[0].cpu().numpy().transpose(1, 2, 0)\n",
    "    img_display = (img_display + 1) / 2  # Denormalize\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, idx].imshow(img_display)\n",
    "    axes[0, idx].set_title(f'Image {idx+1}')\n",
    "    axes[0, idx].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[1, idx].imshow(pred_class, cmap='jet')\n",
    "    axes[1, idx].set_title(f'Prediction {idx+1}')\n",
    "    axes[1, idx].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2, idx].imshow(img_display)\n",
    "    axes[2, idx].imshow(pred_class, alpha=0.5, cmap='jet')\n",
    "    axes[2, idx].set_title(f'Overlay {idx+1}')\n",
    "    axes[2, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/batch_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Batch predictions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf603f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MA-UDA Results Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: Swin Transformer with Meta Attention\")\n",
    "print(f\"Task: T1 -> T2 Brain Tumor Segmentation\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Image Size: 256x256\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nVisualization outputs saved in: ../outputs/\")\n",
    "print(\"- source_prediction.png\")\n",
    "print(\"- domain_comparison.png\")\n",
    "print(\"- attention_maps.png\")\n",
    "print(\"- batch_predictions.png\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
